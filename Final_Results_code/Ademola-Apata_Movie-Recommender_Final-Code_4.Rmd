---
title: "Development of a Movie Recomender"
author: "Ademola Apata"
date: "7/7/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---
## Project Overview

This project focuses on the development of a collborative filtering system (CFR) for recommending movies.

In this project, in order to recommend movies, a large set of users preferences towards different movies is obtained from a publicly available movie rating dataset.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
.libPaths("C:\\Program Files\\R\\R-3.6.2\\library")
```

## Installing packages
```{r libs, warning=FALSE, error=FALSE, message=FALSE}
#install.packages("rlang", type = "source",dependencies = TRUE)
library(data.table) 
#install.packages("recommenderlab")
#install.packages("plyr")
#install.packages("dplyr")
library(dplyr)
library(recommenderlab)
#install.packages("ggplot2",repos = c("http://rstudio.org/_packages","http://cran.rstudio.com"))
library(ggplot2)
#install.packages("reshape2",repos = c("http://rstudio.org/_packages","http://cran.rstudio.com"))
library(reshape2)
```

## Loading Dataset into R
```{r data_load, warning=FALSE, error=FALSE, echo=FALSE}
IMDB_movies <- read.csv("C:/Users/demos/Desktop/Ryerson/Test_IMDB-Dataset/movies.csv", stringsAsFactors = F, header = T)
IMDB_rating <- read.csv("C:/Users/demos/Desktop/Ryerson/Test_IMDB-Dataset/ratings.csv")
```

## Movie Summary and first several rows in a Dataframe
```{r mov_rat_summ, warning=FALSE, error=FALSE, echo=FALSE}
summary(IMDB_movies)
head(IMDB_movies)
summary(IMDB_rating)
head(IMDB_rating)
```

## Data Pre-processing

we need to convert the genres present in the movie_data dataframe into a more usable format by the users. A matrix that comprises of corresponding genres for each of the films was created. Information of movie genres are reorganized from a design perspective to make it much easier for users to be compared with each other from a very long list of movies available in the dataset.

### Extracting a list of genres
```{r data_genres, warning=FALSE, error=FALSE, echo=FALSE}
#Identifying individual movie types in movie dataset
movie_genre <- as.data.frame(IMDB_movies$genres, stringsAsFactors = F)
movie_genre2 <- as.data.frame(tstrsplit(movie_genre[,1], '[|]',type.convert = TRUE), stringsAsFactors = F) 
colnames(movie_genre2) <- c(1:10) #renaming column names 1, 2, 3 etc...

genre_list <- c("Action", "Adventure", "Animation", "Children", 
                "Comedy", "Crime","Documentary", "Drama", "Fantasy",
                "Film-Noir", "Horror", "Musical", "Mystery","Romance",
                "Sci-Fi", "Thriller", "War", "Western")

#Matrix for encoding
genre_matrix <- matrix(0,10330,18)
genre_matrix[1,] <- genre_list
colnames(genre_matrix) <- genre_list

for(index in 1:nrow(movie_genre2)){
  for(col in 1:ncol(movie_genre2)){
    
    col_result = which(genre_matrix[1,] == movie_genre2[index,col]) 
    genre_matrix[index+1, col_result] <- 1
   }
}

genre_matrix2 <- as.data.frame(genre_matrix[-1,], stringsAsFactors = F) #eliminate first row which was on the genre list
for(col in 1:ncol(genre_matrix2)){
  genre_matrix2[,col] <- as.integer(genre_matrix2[,col]) #convert characters to integers
}
head(genre_matrix2)
```

### Create a search matrix for a movie by genre

This will allow us to perform an easy search of the films by specifying the genre present in our movie list.
```{r search_genres, warning=FALSE, error=FALSE, echo=FALSE}
investigate_matrix <- cbind(IMDB_movies[,1:2], genre_matrix2[])
head(investigate_matrix)
str(investigate_matrix)
```

### Converting ratings matrix in a proper format 

Needed in order to use ratings data for building a recommendation engine with recommenderlab library package
```{r rat_mat, warning=FALSE, error=FALSE, echo=FALSE}
movie_ratingMatrix <- dcast(IMDB_rating, userId~movieId, value.var = "rating", na.rm=FALSE)
movie_ratingMatrix <- as.matrix(movie_ratingMatrix[,-1]) #remove 1st column "user Id"

#Convert movie_ratingMatrix into a recommenderlab sparse matrix
movie_ratingMatrix <- as(movie_ratingMatrix, "realRatingMatrix")
movie_ratingMatrix
```

## Exploring Parameters of Recommendation Models
```{r rec_overview, warning=FALSE, error=FALSE, echo=FALSE}
#Load the available options for a recommender model.
recommendation_tools <- recommenderRegistry$get_entries(dataType= "realRatingMatrix")
names(recommendation_tools)
lapply(recommendation_tools,"[[","description") #Explaining the various model types

#Applying Item based collaborative filtering
recommendation_tools$IBCF_realRatingMatrix$parameters
recommendation_tools$UBCF_realRatingMatrix$parameters
```
 
## Exploring Similarity Data - Users

creating a relationship of similarity between the two users. With the help of recommenderlab, we can compute similarities using various operators like cosine, pearson
```{r sim_users, warning=FALSE, error=FALSE, echo=FALSE}
user_similarity <- similarity(movie_ratingMatrix[1:4,],method = "cosine", which = "users")
as.matrix(user_similarity)
image(as.matrix(user_similarity), main = "User Similarities")
```

In the given matrix, each row and each column corresponds to a user, and each cell corresponds to the similarity between two users. The more red the cell is, the more similar two users are. Note that the diagonal is yellow, since it's comparing each user with itself.

## Exploring Similarity Data - Films
```{r sim_movies, warning=FALSE, error=FALSE, echo=FALSE}
movie_similarity <- similarity(movie_ratingMatrix[,1:4], method ="cosine", which = "items")
as.matrix(movie_similarity)
image(as.matrix(movie_similarity), main = "Movie Similarities")
```

Likewise, each row and column corresponds to a movie, and each cell corresponds to the similarity between two movies The more red the cell is, the more similar two movies are. Note that the diagonal is yellow, since it's comparing each user with itself.

## Further data exploration -Exploring the various values of ratings.
```{r rate_values, warning=FALSE, error=FALSE}
ratingValues <-  as.vector(movie_ratingMatrix@data)
unique(ratingValues)  # what are unique values of ratings

movie_rating_table <- table(ratingValues) # what is the count of each rating value
movie_rating_table
```

### Distribution of the ratings

According to the documentation, a rating equal to 0 represents a missing value, so 0 was removed from the dataset before visualizing the results.
```{r dist_ratings, warning=FALSE, error=FALSE, echo=FALSE}
movie_rating_table <- movie_rating_table[-1]
barplot(movie_rating_table, xlab ="vector_ratings", ylab = "count", ylim = c(0,30000), col = "blue")
```

The majority of movies are rated with a score of 3 or higher. The most common rating is 4 as shown in the vector of ratings histogram plot.

### Number of views of the top movies
```{r top_no, warning=FALSE, error=FALSE, echo=FALSE}
Countviews_for_movies <- colCounts(movie_ratingMatrix) #Number of movies

#visual dataframe of movies 
tabular_view_movies <- data.frame(movie = names(Countviews_for_movies), views_count = Countviews_for_movies)#dataframe for view

# sorting table of movies viewed
tabular_view_movies <- tabular_view_movies[order(tabular_view_movies$views_count, decreasing = T),]

#Visual Title of Movies
tabular_view_movies$title <- NA
for (i in 1:10325) {
  tabular_view_movies[i,3] <- as.character(subset(IMDB_movies, IMDB_movies$movieId == tabular_view_movies[i,1])$title)
}
head(tabular_view_movies)
```

## Plot of the total count of the top 6 viewed films.
```{r}
ggplot(tabular_view_movies[1:6, ], aes(x = title, y = views_count)) +
  geom_bar(stat="identity", fill = 'steelblue') +
  geom_text(aes(label=views_count), vjust=-0.3, size=3.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Number of views for the Top Films")
```

"Pulp Fiction (1994)" is the most viewed movie, exceeding the second-most-viewed "Forrest Gump (1994)" by 14 views.

### Distribution of the average movie rating
```{r avg_rat, warning=FALSE, error=FALSE, echo=FALSE, message=FALSE}

average_ratings <- colMeans(movie_ratingMatrix)
qplot(average_ratings) + stat_bin(binwidth = 0.1) + ggtitle("Distribution of the average movie rating")

average_ratings_relevant <- average_ratings[Countviews_for_movies > 50] 
qplot(average_ratings_relevant) + stat_bin(binwidth = 0.1) + ggtitle(paste("Distribution of the relevant average ratings"))
```

The first image above shows the Distribution of the average movie rating. The highest value is around 3, and there are a few movies whose rating is either 1 or 5. Probably, the reason is that these movies received a rating from a few people only, so we shouldn't take them into account. 

Once movies whose number of views is below a defined threshold of 50 was removed,a subset of only relevant movies was created. The second image above shows the distribution of the relevant average ratings. All the rankings are between 2.16 and 4.45. As expected, the extremes were removed. The highest value changes, and now it is around 4.

### Heatmap of Movie Ratings
```{r heat_rate, warning=FALSE, error=FALSE, echo=FALSE}
#The first heat map shows the whole matrix of ratings where each row represents users and columns represents movies and colour shade represents ratings
image(movie_ratingMatrix[], axes = FALSE, main = "Whole matrix rating using Heat maps") # hard to read-too many dimensions

#The second chart is built zooming in on the first 20 rows and 25 columns. 
image(movie_ratingMatrix[1:20,1:25], axes = FALSE, main = "",)
```
Since there are too many users and items, the first chart is hard to read. The second chart is built zooming in on the first rows and columns.

Some users saw more movies than the others. So, instead of displaying some random users and items, the most relevant users and items are selected. Thus only the users who have seen many movies and the movies that have been seen by many users are visualized. To identify and select the most relevant users and movies, I follow these steps:

1. Determine the minimum number of movies per user.
2. Determine the minimum number of users per movie.
3. Select the users and movies matching these criteria.
```{r heat_relev, warning=FALSE, error=FALSE, echo=FALSE}
min_n_movies <- quantile(rowCounts(movie_ratingMatrix), 0.99)
min_n_users <- quantile(colCounts(movie_ratingMatrix), 0.99)
print("Minimum number of movies per user:")
min_n_movies
print("Minimum number of users per movie:")
min_n_users
image(movie_ratingMatrix[rowCounts(movie_ratingMatrix) > min_n_movies,colCounts(movie_ratingMatrix) > min_n_users], main = "Heatmap of the top users and movies")
```

## Data Preparation

### Selecting Useful Data

For finding useful data in our dataset, we have set the threshold for the minimum number of users who have rated a film as 50. This is also same for minimum number of views that are per film. This way, we have filtered a list of watched films from least-watched ones.
```{r rel_data, warning=FALSE, error=FALSE, echo=FALSE}
movie_ratingTreshold <- movie_ratingMatrix[rowCounts(movie_ratingMatrix) > 50, colCounts(movie_ratingMatrix) > 50]
movie_ratingTreshold
```

From the above output of ‘movie_ratings’, we observe that there are 420 users and 447 films as opposed to the previous 668 users and 10325 films. We can now delineate our matrix of relevant users as follows 
```{r rel_explore, warning=FALSE, error=FALSE, echo=FALSE}
#visualization of the top 2 percent of users and movies in the new matrix of the most relevant data:
minrated_movies <- quantile(rowCounts(movie_ratingTreshold),0.98)
minimum_user_who_rated <- quantile(colCounts(movie_ratingTreshold),0.98)
image(movie_ratingTreshold[rowCounts(movie_ratingTreshold) > minrated_movies, colCounts(movie_ratingTreshold) > minimum_user_who_rated],main = "Heatmap of the top users and movies")

distributed_rating_average <- rowMeans(movie_ratingTreshold)
qplot(distributed_rating_average) + stat_bin(binwidth = 0.1)  + ggtitle("Distribution of the average rating per user")
```
In the heatmap, some rows are darker than the others. This might mean that some users give higher ratings to all the movies. The distribution of the average rating per user across all the users varies a lot, as the Distribution of the average rating per user chart below shows.

### Data Normalization.

In the case of some users, there can be high ratings or low ratings provided to all of the watched films. This will act as a bias while implementing our model. normalizing our data removes this
```{r}
normalized_movie_ratings<- normalize(movie_ratingTreshold)
sum(rowMeans(normalized_movie_ratings) > 0.00001)

image(normalized_movie_ratings[rowCounts(normalized_movie_ratings) > minrated_movies, colCounts(normalized_movie_ratings) > minimum_user_who_rated],main = "Normalized Ratings of the Top Users")
```
There are still some lines that seem to be more blue or more red in the Normalized Ratings of the Top Users heatmap. The reason is that I am visualizing only the top movies. I have already checked that the average rating is 0 for each user.

### Data Binarization and Heatmap of the top users and movies.
Some recommendation models work on binary data, so it might be useful to binarize the data, that is, define a table containing only 0s and 1s. The 0s will be either treated as missing values or as bad ratings.

```{r}
#visualizing 5 percent portion of each of binarized matrices.
binary_min_movies <- quantile(rowCounts(movie_ratingTreshold),0.95)
binary_min_users <- quantile(colCounts(movie_ratingTreshold),0.95)

good_rated_films <- binarize(movie_ratingTreshold,minRating = 3)

image(good_rated_films[rowCounts(movie_ratingTreshold) > binary_min_movies ,colCounts(movie_ratingTreshold) > binary_min_users], main = "Heatmap of the top users and movies")
```

## Defining training and test sets.

```{r train_test_sets, warning=FALSE, message=FALSE, echo=FALSE}
#Using 80% training set and 20% test set.
data_sample <- sample (x = c(TRUE, FALSE),size= nrow(movie_ratingTreshold),replace = TRUE, prob = c(0.8, 0.2))
training_set <- movie_ratingTreshold[data_sample, ]
testing_set <- movie_ratingTreshold[!data_sample, ]
```

## Item-based Collaborative Filtering Model

## Displaying recommender parameters
Let's have a look at the default parameters of IBCF model. Here, *k* is the number of items to compute the similarities among them in the first step. After, for each item, the algorithm identifies its *k* most similar items and stores the number. *method* is a similarity funtion, which is *Cosine* by default, may also be *pearson*. I create the model using the default parameters of method = Cosine and k=30.
```{r}
recommendation_info_ibcf <- recommenderRegistry$get_entries(dataType ="realRatingMatrix")
recommendation_info_ibcf$IBCF_realRatingMatrix$parameters
```
## Recommender Model types

## Building the IBCF Recommender Model  with training data
```{r}
recommen_model_IBCF <-  Recommender(data = training_set, method = "IBCF", parameter = list(k = 30))
recommen_model_IBCF

model_details_IBCF <- getModel(recommen_model_IBCF)
model_details_IBCF$data
```

## Exploring the recommender model:
```{r explore_IBCF, warning=FALSE, message=FALSE, echo=FALSE}
model_details <- getModel(recommen_model_IBCF)
model_details$description
model_details$k
class(model_details$sim) # this contains a similarity matrix
dim(model_details$sim)
```

## Applying recommender system on the dataset:
```{r}
predicted_recommendations_IBCF <- predict(object = recommen_model_IBCF,newdata = testing_set, n = 10)# predict() function that will identify similar items and will rank them appropriately. 
# Top_recommendations variable initialized to 10, specifying the number of films to each user.

predicted_recommendations_IBCF
```

## Applying the IBCF recommender model on the test set
```{r}
user1 <- predicted_recommendations_IBCF@items[[1]] # recommendation for the first user
movies_user1 <- predicted_recommendations_IBCF@itemLabels[user1]
movies_user2 <- movies_user1
for (index in 1:10){
  movies_user2[index] <- as.character(subset(IMDB_movies,
                                         IMDB_movies$movieId == movies_user1[index])$title)
}
movies_user2
```

```{r recom_matrixIBCF, warning=FALSE, message=FALSE, echo=FALSE}
## Explore results
recommendation_matrix_ibcf <- sapply (predicted_recommendations_IBCF@items, function(x) {as.integer(colnames(movie_ratingTreshold)[x])}) # matrix with the recommendations for each user
dim(recommendation_matrix_ibcf)
recommendation_matrix_ibcf[,1:4] #Here, the columns represent the first 4 users, and the rows are the *movieId* values of recommended 10 movies.
```

The above matrix contain *movieId* of each recommended movie (rows) for the first four users (columns) in our test dataset.
I also compute how many times each movie got recommended and build the related frequency histogram:

It's also possible to define a matrix with the recommendations for each user. I visualize the recommendations for the first four users:
```{r most_recom_moviesIBCF, warning=FALSE, message=FALSE, echo=FALSE}
number_of_items <- factor(table(recommendation_matrix_ibcf))
chart_title <- "Frequency Histogram count of each movie that got recommended or IBCF"
qplot(number_of_items) + ggtitle(chart_title)
number_of_items_sorted <- sort(number_of_items, decreasing = TRUE)
number_of_items_top <- head(number_of_items_sorted, n = 4)
table_top <- data.frame(as.integer(names(number_of_items_top)),
                       number_of_items_top)
for (i in 1:4){
  table_top[i,1] <- as.character(subset(IMDB_movies, 
                                         IMDB_movies$movieId == table_top[i,1])$title)
}

colnames(table_top) <- c("Movie title", "No of items")
head(table_top)
```
IBCF recommends items on the basis of the similarity matrix. It's an eager-learning model, that is, once it's built, it doesn't need to access the initial data. For each item, the model stores the k-most similar, so the amount of information is small once the model is built. This is an advantage in the presence of lots of data.

Most of the movies have been recommended only a few times, and a few movies have been recommended more than 10 times.

### UBCF Recommender Model - Again, let's first check the default parameters of UBCF model
```{r recom_infoUBCF, warning=FALSE, message=FALSE, echo=FALSE}
recommendation_info_ubcf <- recommenderRegistry$get_entries(dataType = "realRatingMatrix")
```

## Building the recommendation system with training data:
```{r apply_UBCF, warning=FALSE, message=FALSE, echo=FALSE}
recommen_model_UBCF <-  Recommender(data = training_set, method = "UBCF", param=list(method="Cosine",nn=25) )
recommen_model_UBCF

model_details_UBCF <- getModel(recommen_model_UBCF)
model_details_UBCF$data
```

## Applying the UBCF recommender model on the test set
```{r}
n_recommended_ubcf <- 10
recc_predicted_ubcf <- predict(object = recommen_model_UBCF,newdata = testing_set, n = n_recommended_ubcf) 
recc_predicted_ubcf
```

## Using UBCF Recommender on User 1
```{r}
ubcf_user1 <- recc_predicted_ubcf@items[[1]] # recommendation for the first user
movies_ubcf_user1 <- recc_predicted_ubcf@itemLabels [ubcf_user1]
movies_ubcf_user2 <-  movies_ubcf_user1

for (index in 1:10) { movies_ubcf_user2[index] <- as.character(subset(IMDB_movies, IMDB_movies$movieId == movies_ubcf_user1[index])$title)
}
movies_ubcf_user2
```

## Explore results
Let's take a look at the first four users:
```{r}
recommendation_matrix_ubcf <- sapply( recc_predicted_ubcf@items, function(x){as.integer(colnames(movie_ratingTreshold)[x])})

dim(recommendation_matrix_ubcf)
recommendation_matrix_ubcf[, 1:4]
```

The above matrix contain *movieId* of each recommended movie (rows) for the first four users (columns) in our test dataset.

I also compute how many times each movie got recommended and build the related frequency histogram:
```{r times_per_movie, warning=FALSE, message=FALSE, echo=FALSE}
number_of_items_ubcf <- factor(table(recommendation_matrix_ubcf))
chart_title <- "Frequency Histogram count of each movie that got recommended or UBCF"
qplot(number_of_items_ubcf) + ggtitle(chart_title)
```

Compared with the IBCF, the distribution has a longer tail. This means that there are some movies that are recommended much more often than the others. The maximum is more than 30, compared to 10-ish for IBCF.

Let's take a look at the top titles:
```{r top_titles_UBCF, warning=FALSE, message=FALSE, echo=FALSE}
number_of_items_sorted_ubcf <- sort(number_of_items_ubcf, decreasing = TRUE)
number_of_ubcf_items_top <- head(number_of_items_sorted_ubcf, n = 4)
ubcf_table_top <- data.frame(as.integer(names(number_of_ubcf_items_top)), number_of_ubcf_items_top)
for (i in 1:4){
  ubcf_table_top[i,1] <- as.character(subset(IMDB_movies, 
                                         IMDB_movies$movieId == ubcf_table_top[i,1])$title)
}
colnames(ubcf_table_top) <- c("Movie title", "No of items")
head(ubcf_table_top)
```

Comparing the results of UBCF with IBCF helps find some useful insight on different algorithms. UBCF needs to access the initial data. Since it needs to keep the entire database in memory, it doesn't work well in the presence of a big rating matrix. Also, building the similarity matrix for IBCF requires a lot of computing power and time.

## Evaluating the Recommender Systems
We need two trainig and testing data to evaluate the model. There are several methods to create them: 1) splitting the data into training and test sets, 2) bootstrapping, 3) using k-fold.

## Training and Test set split Recommender system.
```{r split_parameters, message=FALSE, warning=FALSE}
percentage_training <- 0.8

#For each user in the test set, we need to define how many items to use to generate recommendations. For this, I first check the minimum number of items rated by users to be sure there will be no users with no items to test.
min(rowCounts(movie_ratingTreshold))
items_to_keep <- 5 #number of items to generate recommendations
rating_threshold <- 3 # threshold with the minimum rating that is considered good
n_eval <- 1

eval_sets <- evaluationScheme(data = movie_ratingTreshold, method = "split",train = percentage_training, given = items_to_keep, goodRating = rating_threshold, k = n_eval) 
eval_sets

getData(eval_sets, "train") # training set
getData(eval_sets, "known") # set with the items used to build the recommendations
getData(eval_sets, "unknown") # set with the items used to test the recommendations
qplot(rowCounts(getData(eval_sets, "unknown"))) + geom_histogram(binwidth = 10) +  ggtitle("unknown items by the users")
#The "unknown items by users" plot displays the unknown items by the users, which varies a lot.
```

## Bootstrapping the data
```{r bootstrap, message=FALSE, warning=FALSE}
eval_sets <- evaluationScheme(data = movie_ratingTreshold, method = "bootstrap", train = percentage_training, given = items_to_keep, goodRating = rating_threshold, k = n_eval)

table_train <- table(eval_sets@runsTrain[[1]])
n_repetitions <- factor(as.vector(table_train))

qplot(n_repetitions) + ggtitle("Number of repetitions in the training set")
# The boottrap chart shows that most of the users have been sampled fewer than four times.
```

## Using cross-validation to validate models

The k-fold cross-validation approach is the most accurate one, although it's computationally heavier. 

Using this approach, we split the data into some chunks, take a chunk out as the test set, and evaluate the accuracy. Then, we can do the same with each other chunk and compute the average accuracy.
```{r k-fold, message=FALSE, warning=FALSE}
n_fold <- 4
eval_sets <- evaluationScheme(data = movie_ratingTreshold, 
                              method = "cross-validation",
                              k = n_fold, 
                              given = items_to_keep, 
                              goodRating = rating_threshold)
size_sets <- sapply(eval_sets@runsTrain, length)
size_sets #Using 4-fold approach, we get four sets of the same size 315.
```

## Evaluating the ratings

For this project, I proceeded with using the k-fold cross validation approach to Evaluate the ratings.
```{r eval_ratings, message=FALSE, warning=FALSE, echo=FALSE}
eval_sets <- evaluationScheme(data = movie_ratingTreshold, 
                              method = "cross-validation", 
                              k = n_fold, 
                              given = items_to_keep, 
                              goodRating = rating_threshold) #First, I re-define the evaluation sets, build IBCF model and create a matrix with predicted ratings

model_to_evaluate <- "UBCF" #Can be changed to IBCF or UBCF to evaluate model performance
model_parameters <- NULL

eval_recommender <- Recommender(data = getData(eval_sets, "train"), 
                                method = model_to_evaluate, 
                                parameter = model_parameters)

items_to_recommend <- 10
eval_prediction <- predict(object = eval_recommender, 
                           newdata = getData(eval_sets, "known"), 
                           n = items_to_recommend, 
                           type = "ratings")
# qplot(rowCounts(eval_prediction)) + 
#  geom_histogram(binwidth = 10) +
#  ggtitle("Distribution of movies per user") #The "Distribution of movies per user" image below displays the distribution of movies per user in the matrix of predicted ratings.
```

Now, I compute the accuracy measures for each user. Most of the RMSEs (Root mean square errors) are in the range of 0.5 to 1.8:
```{r acc, message=FALSE,  warning=FALSE, echo=FALSE}
# Computing the acccuracy of each user with the Root Mean Square Error
 eval_accuracy <- calcPredictionAccuracy( x = eval_prediction, 
                                          data = getData(eval_sets, "unknown"), 
                                          byUser = TRUE)
# In order to have a performance index for the whole model, I specify byUser as FALSE and compute the average indices
head(eval_accuracy)

#qplot(eval_accuracy[, "RMSE"]) + 
#  geom_histogram(binwidth = 0.1) +
#  ggtitle("Distribution of the RMSE by user")
```

In order to have a performance index for the whole model, I specify *byUser* as FALSE and compute the average indices:
```{r acc_IBCF/UBCF, message=FALSE,  warning=FALSE, echo=FALSE}
eval_accuracy <- calcPredictionAccuracy(x = eval_prediction, 
                                        data = getData(eval_sets, "unknown"), 
                                        byUser = FALSE) 
eval_accuracy
```

## Evaluating the recommendations

Another way to measure accuracies is by comparing the recommendations with
the purchases having a positive rating. For this, I can make use of a prebuilt
*evaluate* function in *recommenderlab* library. The function evaluate the recommender performance depending on the number *n* of items to recommend to each user. I use *n* as a sequence n = seq(10, 100, 10). 

The first rows of the resulting performance matrix is presented below:
```{r}
# Evaluating the recommendations
results <- evaluate(x = eval_sets, 
                    method = model_to_evaluate, 
                    n = seq(10, 100, 10))
(getConfusionMatrix(results)[[1]])
```

In order to have a look at all the splits at the same time, I sum up the indices of columns TP, FP, FN and TN:
```{r conf_matrix_whole, message=FALSE, warning=FALSE, echo=FALSE}
columns_to_sum <- c("TP", "FP", "FN", "TN")
indices_summed <- Reduce("+", getConfusionMatrix(results))[, columns_to_sum]
head(indices_summed)
```

Finally, I plot the ROC and the precision/recall curves:
```{r roc, message=FALSE, warning=FALSE}
plot(results, annotate = TRUE, main = "UBCF ROC curve")
plot(results, "prec/rec", annotate = TRUE, main = "UBCF Precision-recall")

# If a small percentage of rated movies is recommended, the precision decreases. On the other hand, the higher percentage of rated movies is recommended the higher is the recall.

```

## Comparing models

In order to compare different models, I define them as a following list:

* Item-based collaborative filtering, using the Cosine as the distance function
* Item-based collaborative filtering, using the Pearson correlation as the distance function
* User-based collaborative filtering, using the Cosine as the distance function
* User-based collaborative filtering, using the Pearson correlation as the distance function
* Random recommendations to have a base line
```{r define_diff_models, warning=FALSE, message=FALSE, echo=FALSE}
models_to_evaluate <- list(
IBCF_cos = list(name = "IBCF", 
                param = list(method = "cosine")),
IBCF_cor = list(name = "IBCF", 
                param = list(method = "pearson")),
UBCF_cos = list(name = "UBCF", 
                param = list(method = "cosine")),
#UBCF_cor = list(name = "UBCF", 
#                param = list(method = "pearson")),
random = list(name = "RANDOM", param=NULL)
)
```

Then, I define a different set of numbers for recommended movies (n_recommendations <- c(1, 5, seq(10, 100, 10))), run and evaluate the models:
```{r params, warning=FALSE, message=FALSE, echo=FALSE}
n_recommendations <- c(1, 5, seq(10, 100, 10))
list_results <- evaluate(x = eval_sets, 
                         method = models_to_evaluate, 
                         n = n_recommendations)
sapply(list_results, class) == "evaluationResults"

```


The following table presents as an example the first rows of the performance evaluation matrix for the IBCF with Cosine distance:
```{r ex_compare, warning=FALSE, message=FALSE, echo=FALSE}
avg_matrices <- lapply(list_results, avg)
head(avg_matrices$IBCF_cos[, 5:8])
```

## Identifying the most suitable model

I compare the models by building a chart displaying their ROC curves and Precision/recall curves.
```{r compare_models_roc, message=FALSE, warning=FALSE, echo=FALSE}
plot(list_results, annotate = 1, legend = "topleft") 
title("ROC curve")
plot(list_results, "prec/rec", annotate = 1, legend = "bottomright")
title("Precision-recall")
```

A good performance index is the area under the curve (AUC), that is, the area under
the ROC curve. Even without computing it, the chart shows that the highest is UBCF
with cosine distance, so it's the best-performing technique.

The UBCF with cosine distance is still the top model. Depending on what is the main purpose of the system, an appropriate number of items to recommend should be defined.